{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d19fc63-7c14-4a79-bd0a-a7fcd26f9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4688d9-f617-4306-bdc4-644247c4e601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minio\n",
      "  Downloading minio-7.2.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2023.7.22)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from minio) (2.0.7)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from minio) (23.1.0)\n",
      "Collecting pycryptodome (from minio)\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from minio) (4.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.21)\n",
      "Downloading minio-7.2.7-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycryptodome, minio\n",
      "Successfully installed minio-7.2.7 pycryptodome-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8249bde6-c694-4ca9-b7ac-bcbf2a75a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e53ea77-6e3d-48ac-8357-893906c3c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\"minio:9000\",\n",
    "    access_key=\"DImuDiaEs6bIJJBgCjXW\",\n",
    "    secret_key=\"HPCTwG5dobqO83MAoCz4hmpPMaJblP9RvondmMHJ\",\n",
    "    secure = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a3c814-6b19-4e18-97d7-9ce579dcda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.make_bucket(\"bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17b50b58-b243-4c16-adf7-39c7c33366c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dados/clientes.json enviado para o bucket bronze com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from io import BytesIO\n",
    "\n",
    "# Nome do bucket e do objeto\n",
    "bucket_name = \"bronze\"\n",
    "object_name = \"dados/clientes.json\"  # Caminho e nome do arquivo no bucket\n",
    "\n",
    "# Dados a serem gravados\n",
    "dados = {\n",
    "    \"clientes\": [\n",
    "        {\"nome\": \"João\", \"sobrenome\": \"Silva\", \"idade\": 34, \"email\": \"joao.silva@email.com\", \"data_cadastro\": \"2024-08-09\"},\n",
    "        {\"nome\": \"Maria\", \"sobrenome\": \"Oliveira\", \"idade\": 29, \"email\": \"maria.oliveira@email.com\", \"data_cadastro\": \"2024-08-09\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Converter o dicionário Python em uma string JSON\n",
    "json_data = json.dumps(dados)\n",
    "\n",
    "# Converter a string JSON em bytes para poder enviar ao MinIO\n",
    "json_bytes = BytesIO(json_data.encode('utf-8'))\n",
    "\n",
    "# Fazendo upload do JSON para o MinIO\n",
    "try:\n",
    "    client.put_object(\n",
    "        bucket_name,\n",
    "        object_name,\n",
    "        data=json_bytes,\n",
    "        length=len(json_data),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    "    print(f'JSON {object_name} enviado para o bucket {bucket_name} com sucesso!')\n",
    "except S3Error as e:\n",
    "    print(f'Erro ao enviar arquivo: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6be8af6-8d56-4365-81d1-f41c58b6e46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "71a3bd16-9625-4c97-ae8b-a230ca1aad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aguardando mensagens...\n",
      "Recebido: {\"nome\": \"Enrico\", \"sobrenome\": \"Santos\", \"idade\": 37, \"email\": \"limacamila@example.net\", \"data_cadastro\": \"2024-05-20\"}\n",
      "Recebido: {\"nome\": \"Isaac\", \"sobrenome\": \"Nascimento\", \"idade\": 28, \"email\": \"olivia70@example.org\", \"data_cadastro\": \"2024-05-17\"}\n",
      "Recebido: {\"nome\": \"Brenda\", \"sobrenome\": \"Teixeira\", \"idade\": 37, \"email\": \"beniciofarias@example.org\", \"data_cadastro\": \"2024-06-23\"}\n",
      "Recebido: {\"nome\": \"Arthur\", \"sobrenome\": \"Ramos\", \"idade\": 65, \"email\": \"diogomelo@example.com\", \"data_cadastro\": \"2024-06-25\"}\n",
      "Recebido: {\"nome\": \"Paulo\", \"sobrenome\": \"Nunes\", \"idade\": 52, \"email\": \"sofiada-costa@example.net\", \"data_cadastro\": \"2024-03-14\"}\n",
      "Recebido: {\"nome\": \"Sofia\", \"sobrenome\": \"Santos\", \"idade\": 44, \"email\": \"limathomas@example.com\", \"data_cadastro\": \"2024-03-11\"}\n",
      "Recebido: {\"nome\": \"Thiago\", \"sobrenome\": \"Pires\", \"idade\": 60, \"email\": \"erick72@example.com\", \"data_cadastro\": \"2024-08-07\"}\n",
      "Recebido: {\"nome\": \"Isadora\", \"sobrenome\": \"Ramos\", \"idade\": 69, \"email\": \"meloagatha@example.org\", \"data_cadastro\": \"2024-04-06\"}\n",
      "Recebido: {\"nome\": \"Amanda\", \"sobrenome\": \"Rezende\", \"idade\": 61, \"email\": \"fernando30@example.net\", \"data_cadastro\": \"2024-06-24\"}\n",
      "Recebido: {\"nome\": \"Elisa\", \"sobrenome\": \"Ribeiro\", \"idade\": 57, \"email\": \"marcos-vinicius92@example.net\", \"data_cadastro\": \"2024-05-12\"}\n",
      "Recebido: {\"nome\": \"Emanuel\", \"sobrenome\": \"Ferreira\", \"idade\": 62, \"email\": \"natalia12@example.com\", \"data_cadastro\": \"2024-01-19\"}\n",
      "JSON KAFKA/topic=teste/year=2024/month=8/day=10/2024-08-10 20:59:54.json enviado para o bucket bronze com sucesso!\n",
      "OFFSET 1989 \n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "TOPIC = 'teste'\n",
    "BOOTSTRAP = 'kafka:9092'\n",
    "ACCESS_KEY = \"DImuDiaEs6bIJJBgCjXW\"\n",
    "SECRET_KEY = \"HPCTwG5dobqO83MAoCz4hmpPMaJblP9RvondmMHJ\"\n",
    "from clickhouse_driver import Client\n",
    "from minio import Minio\n",
    "BUCKET_NAME = 'bronze-layer'\n",
    "\n",
    "class TopicOffsetControll():\n",
    "    def __init__(self, topic):\n",
    "        self.topic = topic\n",
    "        self.client = Client(\n",
    "                host='clickhouse',       # Endereço do servidor ClickHouse\n",
    "                port=9000,              # Porta padrão para conexões TCP\n",
    "                user='default',         # Usuário (padrão é 'default')\n",
    "                password='',            # Senha (por padrão, vazio)\n",
    "                database='default',     # Nome do banco de dados (por padrão, 'default')\n",
    "        )\n",
    "        \n",
    "    def get_offset(self):\n",
    "        self.client.execute(\"\"\"CREATE TABLE IF NOT EXISTS kafka_topics_offset_control (\n",
    "                kafka_topic String,\n",
    "                last_offset Int64,\n",
    "                exec_date DEFAULT now() \n",
    "                ) ENGINE = MergeTree()\n",
    "                ORDER BY exec_date \"\"\")\n",
    "        offset = self.client.execute(f\"SELECT max(last_offset) FROM kafka_topics_offset_control WHERE kafka_topic = '{self.topic}'\")[0][0]\n",
    "        return offset\n",
    "\n",
    "    def insert_offset(self, offset):\n",
    "        print(f\"OFFSET {offset} \")\n",
    "        self.client.execute(f\"\"\"\n",
    "        INSERT INTO kafka_topics_offset_control (kafka_topic, last_offset) \n",
    "        VALUES ('{self.topic}',  {offset})\n",
    "        \"\"\")\n",
    "    \n",
    "def insert_bronze(data):\n",
    "    client = Minio(\"minio:9000\",\n",
    "        access_key=ACCESS_KEY,\n",
    "        secret_key=SECRET_KEY,\n",
    "        secure = False\n",
    "    )\n",
    "    found = client.bucket_exists(BUCKET_NAME)\n",
    "    if not found:\n",
    "        client.make_bucket(BUCKET_NAME)\n",
    "        print(\"Created bucket\", BUCKET_NAME)\n",
    "    json_data = json.dumps(dados)\n",
    "    json_bytes = BytesIO(json_data.encode('utf-8'))\n",
    "    now = datetime.now()\n",
    "    object_name = f'KAFKA/topic={TOPIC}/year={now.year}/month={now.month}/day={now.day}/{now.strftime(\"%Y-%m-%d %H:%M:%S\")}.json'\n",
    "    try:\n",
    "        client.put_object(\n",
    "            BUCKET_NAME,\n",
    "            object_name,\n",
    "            data=json_bytes,\n",
    "            length=len(json_data),\n",
    "            content_type=\"application/json\"\n",
    "        )\n",
    "        print(f'JSON {object_name} enviado para o bucket {bucket_name} com sucesso!')\n",
    "    except S3Error as e:\n",
    "        print(f'Erro ao enviar arquivo: {e}')    \n",
    "# Configuração do consumidor Kafka\n",
    "try:\n",
    "    consumer = KafkaConsumer(\n",
    "        bootstrap_servers=[BOOTSTRAP],  # Servidor Kafka\n",
    "        enable_auto_commit=True,  # Para commit automático das mensagens\n",
    "        consumer_timeout_ms=10000,  #fica aguardando topicos por 10 segundos\n",
    "        value_deserializer=lambda x: x.decode('utf-8')  # Deserializa as mensagens para string\n",
    "    )\n",
    "    # Especificar a partição e o offset inicial\n",
    "    topic_controll = TopicOffsetControll(TOPIC)\n",
    "    partition = 0  # Partição que deseja consumir\n",
    "    offset_inicial = topic_controll.get_offset()  # Offset a partir do qual deseja começar a ler\n",
    "    \n",
    "    # Atribuir a partição ao consumidor\n",
    "    tp = TopicPartition(TOPIC, partition)\n",
    "    consumer.assign([tp])\n",
    "    \n",
    "    # Definir o offset inicial manualmente\n",
    "    consumer.seek(tp, offset_inicial)\n",
    "    \n",
    "    print(\"Aguardando mensagens...\")\n",
    "    last_message = None\n",
    "    # Consome mensagens do tópico\n",
    "    message_list = []\n",
    "    for message in consumer:\n",
    "        last_message = message\n",
    "        print(f\"Recebido: {message.value}\")\n",
    "        message_list.append(message.value)\n",
    "        \n",
    "    insert_bronze(message.value)\n",
    "    topic_controll.insert_offset(message.offset)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise Exception(e)\n",
    "    #create_log(status = 'failure', message = e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc674c5e-bcb7-4638-8b46-705e811bd75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1616d0-a393-44f0-9c9b-e9d25c5a00bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba223585-4842-4b9a-936d-b94f9344f0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clickhouse_driver in /opt/conda/lib/python3.11/site-packages (0.2.8)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from clickhouse_driver) (2023.3.post1)\n",
      "Requirement already satisfied: tzlocal in /opt/conda/lib/python3.11/site-packages (from clickhouse_driver) (5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install clickhouse_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc3b1f0-d5fa-4bba-bb88-98b8842523dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo do arquivo KAFKA/topic=teste/year=2024/month=8/day=10/2024-08-10 21:45:28.json:\n",
      "{\"nome\": \"Ana J\\u00falia\", \"sobrenome\": \"Barbosa\", \"idade\": 31, \"email\": \"cavalcantimaria-alice@example.net\", \"data_cadastro\": \"2024-02-20\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "from clickhouse_driver import Client\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import json\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "TOPIC = 'clients_kafka'\n",
    "BOOTSTRAP = 'kafka:9092'\n",
    "ACCESS_KEY = 'minioadmin'\n",
    "SECRET_KEY = 'minioadmin'\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "file_path = 'KAFKA/topic=teste/year=2024/month=8/day=10/2024-08-10 21:45:28.json'\n",
    "from minio import Minio\n",
    "BUCKET_NAME = 'bronze-layer'\n",
    "client = Minio(\"minio:9000\",\n",
    "    access_key=ACCESS_KEY,\n",
    "    secret_key=SECRET_KEY,\n",
    "    secure = False\n",
    ")\n",
    "\n",
    "data = client.get_object(BUCKET_NAME, file_path)\n",
    "content = json.loads(data.read().decode('utf-8'))\n",
    "print(f\"Conteúdo do arquivo {file_path}:\")\n",
    "print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "598c6ea1-601d-4c06-912b-d5bd272a73c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"nome\": \"Ana J\\\\u00falia\", \"sobrenome\": \"Barbosa\", \"idade\": 31, \"email\": \"cavalcantimaria-alice@example.net\", \"data_cadastro\": \"2024-02-20\"}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ac6dd7f-4c4f-4c73-af9d-11bb67432d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client\n",
    "\n",
    "# Conexão ao servidor ClickHouse\n",
    "client = Client(\n",
    "    host='clickhouse',       # Endereço do servidor ClickHouse\n",
    "    port=9000,              # Porta padrão para conexões TCP\n",
    "    user='default',         # Usuário (padrão é 'default')\n",
    "    password='',            # Senha (por padrão, vazio)\n",
    "    database='default',     # Nome do banco de dados (por padrão, 'default')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3a4d1b5d-a17d-463c-a3e5-02439101abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\"\"\"CREATE OR REPLACE TABLE kafka_topics_offset_control (\n",
    "                id UInt64,\n",
    "                kafka_topic String,\n",
    "                last_offset Int64,\n",
    "                exec_date DateTime\n",
    "                ) ENGINE = MergeTree()\n",
    "                ORDER BY id \"\"\"\n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9de85d53-2237-43c8-a96b-715d90e974b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\"\"\"INSERT INTO kafka_topics_offset_control (id, kafka_topic, last_offset, exec_date) VALUES \n",
    "(1, 'topic1', 12345, '2024-08-10'),\n",
    "(2, 'topic2', 67890, '2024-08-10'),\n",
    "(3, 'topic1', 12350, '2024-08-11'),\n",
    "(4, 'topic3', 54321, '2024-08-11'),\n",
    "(5, 'topic2', 67895, '2024-08-12');\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "12017898-bc90-4af2-b068-13c56ea172f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teste', 1979, datetime.datetime(2024, 8, 10, 20, 59, 1)),\n",
       " ('teste', 1989, datetime.datetime(2024, 8, 10, 20, 59, 54))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(f\"SELECT * FROM  kafka_topics_offset_control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77126a19-f792-40bc-b4d8-bf248d15a2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "client.execute(f\"SELECT max(last_offset) FROM kafka_topics_offset_control WHERE kafka_topic = 'TOPIC'\")[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206303e-d54e-4e1f-ab61-cfb8091fb068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
